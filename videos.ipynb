{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from utilities import *\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('user_and_course_dfs.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run other_graphing_utilities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_video_df(df):\n",
    "    df.video_id = df.video_id.str.replace('video_', '')\n",
    "    df.section = df.section.str.replace('\\d+-', '')\n",
    "\n",
    "    bracket_regex = re.compile(r'\\[.*\\],*\\s*')\n",
    "    output_regex = re.compile(r'>>\\s*')\n",
    "\n",
    "    all_missed_data = []\n",
    "    all_scores = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if not isinstance(row.speech_times, list): \n",
    "            all_missed_data.append([])\n",
    "            all_scores.append([])\n",
    "            continue\n",
    "\n",
    "        missed_data = []\n",
    "        filtered_lines = []\n",
    "        filtered_speech_times = []\n",
    "        \n",
    "        for i in range(len(row.transcript_en)):\n",
    "            line = row.transcript_en[i]\n",
    "\n",
    "            brackets = re.findall(bracket_regex, line)\n",
    "            outputs = re.findall(output_regex, line)\n",
    "            missed_data.extend(brackets)\n",
    "            missed_data.extend(outputs)\n",
    "\n",
    "            for exclude in brackets + outputs:\n",
    "                line = line.replace(exclude, '', 1)\n",
    "\n",
    "            if line: \n",
    "                filtered_lines.append(line)\n",
    "                filtered_speech_times.append(row.speech_times[i])\n",
    "\n",
    "        scores = [textstat.dale_chall_readability_score(l) for l in filtered_lines]\n",
    "\n",
    "        assert len(filtered_lines) == len(filtered_speech_times) == len(scores)\n",
    "\n",
    "        row.transcript_en = filtered_lines\n",
    "        row.speech_times = filtered_speech_times\n",
    "\n",
    "        all_scores.append(scores)\n",
    "        all_missed_data.append(missed_data)\n",
    "\n",
    "    assert len(all_missed_data) == len(all_scores) == df.shape[0]\n",
    "\n",
    "    df['missed_data'] = all_missed_data\n",
    "    df['readability_scores'] = all_scores\n",
    "    \n",
    "def get_video_info(video, sections_viewed):\n",
    "    video_row = videos_with_transcript[videos_with_transcript.video_id == video]\n",
    "    if not video_row.size: return\n",
    "    \n",
    "    speech_times = video_row.iloc[0].speech_times\n",
    "    graph_sections = list(np.array(speech_times).flatten())\n",
    "\n",
    "    for s in sections_viewed:\n",
    "        graph_sections.extend(s[0:2])\n",
    "\n",
    "    graph_sections.append(0)\n",
    "    graph_sections = list(set(graph_sections))\n",
    "    graph_sections.sort()\n",
    "\n",
    "    views = np.zeros(len(graph_sections))\n",
    "    stops = np.zeros(len(graph_sections))\n",
    "    seek_forwards = np.zeros(len(graph_sections))\n",
    "    seek_backwards = np.zeros(len(graph_sections))\n",
    "    pauses = np.zeros(len(graph_sections))\n",
    "    \n",
    "    for section in sections_viewed:\n",
    "        # No bar starting at the last graph edge\n",
    "        for i in range(len(graph_sections) - 1):\n",
    "            if section[0] <= graph_sections[i] < section[1]:\n",
    "                views[i] += 1\n",
    "    \n",
    "    for section in sections_viewed:\n",
    "        end = section[1]\n",
    "        \n",
    "        i = graph_sections.index(end)\n",
    "        \n",
    "        assert(i != 0)\n",
    "        \n",
    "        if section[2] == 'stop':\n",
    "            stops[i] += 1\n",
    "        elif section[2] == 'seek':\n",
    "            if section[3] == FORWARD:\n",
    "                seek_forwards[i] += 1\n",
    "            elif section[3] == BACKWARD:\n",
    "                seek_backwards[i] += 1\n",
    "            else:\n",
    "                print(\"Invalid seek event!\")\n",
    "        elif section[2] == 'pause':\n",
    "            pauses[i] += 1\n",
    "        else:\n",
    "            print(\"Invalid stop event!\")\n",
    "    \n",
    "    # Scores are guaranteed to be greater than zero\n",
    "    # because the equation includes + 0.0496 (word/sentences)\n",
    "    # and every transcript line has at least one word.\n",
    "    # This is necessary because not every section of the video\n",
    "    # has a corresponding transcript\n",
    "    section_scores = [-1 for _ in range(len(graph_sections))]\n",
    "    \n",
    "    complexity_scores = video_row['readability_scores'].iloc[0]\n",
    "    \n",
    "    for score in complexity_scores:\n",
    "        assert score > 0\n",
    "\n",
    "    assert len(complexity_scores) <= len(graph_sections)\n",
    "\n",
    "    for i in range(len(graph_sections)): \n",
    "        for j in range(len(speech_times)):\n",
    "            if graph_sections[i] >= speech_times[j][0] and graph_sections[i] <= speech_times[j][1]:\n",
    "                section_scores[i] = complexity_scores[j]\n",
    "                break\n",
    "    \n",
    "    return graph_sections, section_scores, views, \\\n",
    "           video_row.start.iloc[0], stops, seek_forwards, \\\n",
    "           seek_backwards, pauses, video_row.video_duration.iloc[0], \\\n",
    "           video_row.unit.iloc[0]\n",
    "\n",
    "def get_spikes(spike_threshold):\n",
    "    all_spikes = []\n",
    "\n",
    "    for video, sections_viewed in sections_aggregated.items():\n",
    "        \n",
    "        video_info = get_video_info(video, sections_viewed)\n",
    "\n",
    "        try:\n",
    "            graph_sections, section_scores, views, start, _, _, _, _, end, _ = video_info\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "        df_len = len(section_scores)\n",
    "\n",
    "        assert df_len == len(views) == len(graph_sections)\n",
    "        \n",
    "        score_views_df = pd.DataFrame(zip(section_scores, views, \n",
    "                                          [s for s in graph_sections[:-1]],\n",
    "                                          [s for s in graph_sections[1:]],\n",
    "                                          [video] * df_len,\n",
    "                                          [start] * df_len), \n",
    "                                      columns=['score', 'views', 'section_start', 'section_end', 'video', 'start'])\n",
    "\n",
    "        if end:\n",
    "            score_views_df['end'] = [(end if end else None) for _ in range(score_views_df.shape[0])]\n",
    "        \n",
    "        spikes = score_views_df[score_views_df['views'] > spike_threshold]\n",
    "    \n",
    "        # remove sections with no complexity score\n",
    "        spikes = spikes[spikes['score'] > 0]\n",
    "        \n",
    "        all_spikes.append(spikes)\n",
    "\n",
    "    all_spikes = pd.concat(all_spikes, sort=False)\n",
    "\n",
    "    elementary_spikes = all_spikes[all_spikes['score'] < ELEMENTARY]\n",
    "    harder_spikes = all_spikes[all_spikes['score'] >= ELEMENTARY]\n",
    "\n",
    "    print(\"elementary: \" + str(elementary_spikes.shape[0]), \"harder: \" + str(harder_spikes.shape[0]))\n",
    "    return elementary_spikes, harder_spikes    \n",
    "\n",
    "def get_sections_viewed(data):\n",
    "    sections_per_user = collections.defaultdict(list)\n",
    "    sections_aggregated = collections.defaultdict(list)\n",
    "\n",
    "    invalid_data = []\n",
    "\n",
    "    for u in data:\n",
    "        u = u[u.event_type.isin(INTERESTING_EVENTS)]\n",
    "        sections = collections.defaultdict(list)\n",
    "\n",
    "        if not u.size:\n",
    "            continue\n",
    "\n",
    "        assert u.time.is_monotonic_increasing\n",
    "\n",
    "        for i in range(0, u.shape[0] - 1):\n",
    "            r1 = u.iloc[i]\n",
    "            r2 = u.iloc[i + 1]\n",
    "\n",
    "            r1_id = get_event_id(r1)\n",
    "\n",
    "            if r1.event_type == 'play_video' and \\\n",
    "                r2.event_type in STOP_EVENTS and \\\n",
    "                r1_id == get_event_id(r2):\n",
    "\n",
    "                    start = get_event_dict(r1)['currentTime']\n",
    "\n",
    "                    try:\n",
    "                        next_event_dict = get_event_dict(r2)\n",
    "                        end = next_event_dict['currentTime']\n",
    "\n",
    "                    except KeyError:\n",
    "                        end = next_event_dict['old_time']\n",
    "\n",
    "                    # Some seek_events are invalid\n",
    "                    if start >= end:\n",
    "                        invalid_data.append((r1, r2, u.iloc[0].user))\n",
    "                        continue\n",
    "\n",
    "                    stop_type = r2.event_type.split('_')[0] \n",
    "                    section = [start, end, stop_type]\n",
    "                    if stop_type == 'seek':\n",
    "                        if next_event_dict['new_time'] > start:\n",
    "                            section.append(FORWARD)\n",
    "                        else:\n",
    "                            section.append(BACKWARD)\n",
    "\n",
    "                    sections[r1_id].append(section)\n",
    "                    sections_aggregated[r1_id].append(section)\n",
    "\n",
    "        sections_per_user[u.iloc[0].user] = sections\n",
    "        \n",
    "    return sections_per_user, sections_aggregated, invalid_data\n",
    "def graph_complexity(video, sections_viewed):\n",
    "    video_info = get_video_info(video, sections_viewed)\n",
    "    \n",
    "    try:\n",
    "        graph_sections, section_scores, views, start, _, _, _, _, end, unit = video_info\n",
    "    except TypeError:\n",
    "        return\n",
    "    \n",
    "    min_score = min(i for i in section_scores if i > 0)\n",
    "    max_score = max(section_scores)\n",
    "    norm = plt.Normalize(min_score, max_score)\n",
    "    CMAP_2.set_under('black')\n",
    "    colors = CMAP_2(norm(section_scores))\n",
    "    \n",
    "    sm = plt.cm.ScalarMappable(cmap=CMAP_2, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    ticks = np.linspace(min_score, max_score, num=N_TICKS)\n",
    "    tick_labels = [str(t) for t in ticks]\n",
    "    \n",
    "    cbar = plt.colorbar(sm, ticks=ticks)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.ax.set_ylabel('Readability Score')\n",
    "\n",
    "    df = pd.DataFrame(zip(graph_sections, views), columns=['time', 'views']).set_index('time')\n",
    "    \n",
    "    ax = df['views'].plot.bar(color=colors, align='edge', width=1, ec='white', figsize=(40, 5))\n",
    "\n",
    "    actual_start = min(graph_sections, key=lambda x: abs(x - start))\n",
    "    start_index = graph_sections.index(actual_start)\n",
    "    ax.axvline(x=start_index, color='r', linestyle='--')\n",
    "    \n",
    "    if end:\n",
    "        end_index = min(graph_sections, key=lambda x:abs(x-end))\n",
    "        end_index = graph_sections.index(end_index)\n",
    "        ax.axvline(x=end_index, color='r', linestyle='--')\n",
    "\n",
    "    ax.set(xlabel='time', ylabel='# views', title=video + \" unit \" + unit)\n",
    "    \n",
    "    save_or_display('video views for ' + video + ' unit ' + unit)\n",
    "    \n",
    "def graph_stop_events(video, sections_viewed):\n",
    "    video_info = get_video_info(video, sections_viewed)\n",
    "    \n",
    "    try:\n",
    "        graph_sections, section_scores, views, start, stops, \\\n",
    "        seek_forwards, seek_backwards, pauses, end, unit = video_info\n",
    "    except TypeError:\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(zip(graph_sections, stops, seek_forwards, seek_backwards, pauses), \n",
    "                      columns=['time', 'stops', 'seek_forwards', \\\n",
    "                               'seek_backwards', 'pauses']).set_index('time')\n",
    "    \n",
    "    interval = 5\n",
    "    max_edge = max(graph_sections)\n",
    "    if max_edge % interval != 0:\n",
    "        max_edge += interval\n",
    "        \n",
    "    bins = np.arange(0, max_edge, interval)\n",
    "    df = df.groupby(pd.cut(df.index, bins)).sum() \n",
    "    ax = df.plot(kind='bar', stacked=True, title=video + ' unit ' + unit, ec='white', figsize=(20, 5))\n",
    "        \n",
    "    binned_complexities = []\n",
    "    for i in range(len(bins) - 1):\n",
    "        start, end = bins[i], bins[i + 1]\n",
    "        sections = [i for i in range(len(graph_sections)) if start <= graph_sections[i] < end]\n",
    "        section_complexities = [section_scores[i] for i in sections]\n",
    "        if section_complexities:\n",
    "            max_section_complexity = max(section_complexities)\n",
    "            binned_complexities.append(max_section_complexity)\n",
    "        else:\n",
    "            binned_complexities.append(-1)\n",
    "    \n",
    "    min_score = min(i for i in binned_complexities if i > 0)\n",
    "    max_score = max(binned_complexities)\n",
    "    norm = plt.Normalize(min_score, max_score)\n",
    "    CMAP_2.set_under('black')\n",
    "    colors = CMAP_2(norm(binned_complexities))\n",
    "    sm = plt.cm.ScalarMappable(cmap=CMAP_2, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    ticks = np.linspace(min_score, max_score, num=N_TICKS)\n",
    "    tick_labels = [str(t) for t in ticks]\n",
    "    \n",
    "    cbar = plt.colorbar(sm, ticks=ticks)\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "    cbar.ax.set_ylabel('Max Readability Score')\n",
    "    \n",
    "    ticklabels = ax.get_xticklabels(which='both')\n",
    "    for i in range(len(ticklabels)):\n",
    "        ticklabels[i].set_color(colors[i])\n",
    "    \n",
    "    save_or_display('stop events for ' + video + ' unit ' + unit)\n",
    "\n",
    "def get_event_id(row):\n",
    "    return get_event_dict(row)['id']\n",
    "\n",
    "def get_event_dict(row):\n",
    "    return json.loads(row.event)\n",
    "\n",
    "def get_all_starts(data):\n",
    "    all_starts = collections.defaultdict(list)\n",
    "    for u in data:\n",
    "        u = u[u.event_type.isin(['play_video', 'load_video'])]\n",
    "\n",
    "        for i in range(0, u.shape[0] - 1):\n",
    "            r1 = u.iloc[i]\n",
    "            r2 = u.iloc[i + 1]\n",
    "            r1_id = get_event_id(r1)\n",
    "            if r1.event_type == 'load_video' and \\\n",
    "               r2.event_type == 'play_video' and \\\n",
    "                r1_id == get_event_id(r2):\n",
    "\n",
    "                start = get_event_dict(r2)['currentTime']\n",
    "                all_starts[r1_id].append(start)\n",
    "    \n",
    "    return all_starts\n",
    "\n",
    "def get_seeks_info(sections_aggregated):\n",
    "    all_seek_forwards = 0\n",
    "    all_seek_backwards = 0\n",
    "    all_seek_forwards_ls = []\n",
    "    all_seek_backwards_ls = []\n",
    "    all_seek_forwards_ratios = []\n",
    "    all_seek_backwards_ratios = []\n",
    "\n",
    "    for video, sections_viewed in sections_aggregated.items():\n",
    "        video_info = get_video_info(video, sections_viewed)\n",
    "        try:\n",
    "            _, _, _, start, _, \\\n",
    "            seek_forwards, seek_backwards, _, end, unit = video_info\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "        summed_forwards = sum(list(seek_forwards))\n",
    "        summed_backwards = sum(list(seek_backwards))\n",
    "        summed_total = summed_forwards + summed_backwards\n",
    "\n",
    "        all_seek_forwards += summed_forwards\n",
    "        all_seek_backwards += summed_backwards\n",
    "\n",
    "        all_seek_forwards_ls.append(summed_forwards)\n",
    "        all_seek_backwards_ls.append(summed_backwards)\n",
    "\n",
    "\n",
    "        all_seek_forwards_ratios.append(100*(summed_forwards / summed_total) if summed_total else 0)\n",
    "        all_seek_backwards_ratios.append(100*(summed_backwards / summed_total) if summed_total else 0)\n",
    "\n",
    "    print(all_seek_forwards / all_seek_backwards)\n",
    "    print(np.median(all_seek_forwards_ls), np.median(all_seek_backwards_ls))\n",
    "    print(np.median(all_seek_forwards_ratios), np.median(all_seek_backwards_ratios))\n",
    "    \n",
    "    sa_len = 0\n",
    "    for value in sections_aggregated.values():\n",
    "        sa_len += len(value)\n",
    "\n",
    "    invalid_len = len(invalid_data)\n",
    "    print(invalid_len/(invalid_len + sa_len))\n",
    "    \n",
    "    return all_seek_forwards, all_seek_backwards, all_seek_forwards_ls, \\\n",
    "           all_seek_backwards_ls, all_seek_forwards_ratios, all_seek_backwards_ratios\n",
    "    \n",
    "def get_difficult_sections(sections_aggregated):\n",
    "    for video, sections_viewed in sections_aggregated.items():\n",
    "        video_info = get_video_info(video, sections_viewed)\n",
    "\n",
    "        try:\n",
    "            _, section_scores, views, start, _, _, _, _, _,_ = video_info\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "        score_views_df = pd.DataFrame(zip(section_scores, views, \n",
    "                                          [x[0] for x in sections_viewed],\n",
    "                                          [x[1] for x in sections_viewed],\n",
    "                                          [video] * len(views),\n",
    "                                          [start] * len(views)), \n",
    "                                      columns=['score', 'views', 'section_start', \n",
    "                                               'section_end', 'video', 'start'])\n",
    "\n",
    "        shouldnt_be_this_hard = score_views_df[9 <= score_views_df['score']]\n",
    "\n",
    "        if shouldnt_be_this_hard.size:\n",
    "            print(video, shouldnt_be_this_hard)\n",
    "            \n",
    "def graph_dist_seeks(forwards_ratios, backwards_ratios, no_title=False):\n",
    "    title = \"Distribution of forward seeks and backward seeks per video\"\n",
    "    if no_title:\n",
    "        title = \"\"\n",
    "    \n",
    "    seeks_df = pd.DataFrame([forwards_ratios, backwards_ratios], \n",
    "                            index=['forwards', 'backwards']).T\n",
    "    ax = seeks_df.plot(kind='hist', stacked=True, ec='white')\n",
    "    ax.set(title=title, xlabel='Percentage of Total Seeks For Each Video')\n",
    "    save_plot(\"Distribution of Seeks\", graph_type=\"\")\n",
    "#     save_or_display(title)\n",
    "    \n",
    "def plot_spikes(sections_aggregated):\n",
    "    all_find_spike_views = []\n",
    "\n",
    "    for video, sections_viewed in sections_aggregated.items():\n",
    "        video_info = get_video_info(video, sections_viewed)\n",
    "\n",
    "        try:\n",
    "            _, _, views, _, _, _, _, _ , _, _ = video_info\n",
    "        except TypeError:\n",
    "            continue\n",
    "\n",
    "        find_spike_views = views[np.where(views > 2)]\n",
    "        all_find_spike_views.extend(find_spike_views)\n",
    "\n",
    "    views_hist_df = pd.DataFrame(all_find_spike_views)\n",
    "    ax = views_hist_df.plot(kind='hist', bins=80, ec='white', figsize=(20, 5))\n",
    "    ax.set_xlabel('Views')\n",
    "    \n",
    "def plot_start_times(all_starts):\n",
    "    for video, starts in all_starts.items():\n",
    "        video_row = videos_with_transcript[videos_with_transcript.video_id == video]\n",
    "\n",
    "        starts = [int(i) for i in starts]\n",
    "        s = pd.Series(starts).sort_values()\n",
    "\n",
    "        bins = max(max(starts) - min(starts), 1)\n",
    "\n",
    "        ax = s.plot(kind='hist', ec='white', title=video, bins=bins)\n",
    "        ax.set_xlabel('start time')\n",
    "\n",
    "        start_row = video_row.start\n",
    "        if start_row.size:\n",
    "            actual_start = int(start_row.iloc[0])\n",
    "            start_index = starts.index(actual_start)\n",
    "            ax.axvline(x=start_index, color='r', linestyle='--')\n",
    "            actual_start = 0\n",
    "\n",
    "        save_or_display('start times for students for video ' + video)\n",
    "        \n",
    "def plot_dist_of_starts(all_starts):\n",
    "    dists_from_start = []\n",
    "\n",
    "    for video, starts in all_starts.items():\n",
    "        video_row = videos_with_transcript[videos_with_transcript.video_id == video]\n",
    "\n",
    "        start_row = video_row.start\n",
    "\n",
    "        if start_row.size:\n",
    "            actual_start = int(start_row.iloc[0])\n",
    "            dists = [int(i)-actual_start for i in starts]\n",
    "\n",
    "            dists_from_start.extend(dists)\n",
    "\n",
    "    df = pd.DataFrame(dists_from_start)\n",
    "    print(min(dists_from_start))\n",
    "    df.plot(kind='hist', ec='white', bins=23)\n",
    "\n",
    "    save_or_display(\"Distribution of when users start video relative to the actual start of the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.read_json(VIDEO_JSON, orient='records').T.drop('speech_period', axis=1)\n",
    "filter_video_df(videos_df)\n",
    "all_video_ids = list(videos_df.video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_transcript = videos_df[~videos_df.speech_times.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_per_user, sections_aggregated, invalid_data = get_sections_viewed(user_urls_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_starts = get_all_starts(user_urls_dfs)\n",
    "all_seek_forwards, all_seek_backwards, all_seek_forwards_ls, \\\n",
    "all_seek_backwards_ls, all_seek_forwards_ratios, \\\n",
    "all_seek_backwards_ratios = get_seeks_info(sections_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_difficult_sections(sections_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_file = False\n",
    "for video, sections_viewed in sections_aggregated.items():\n",
    "    graph_complexity(video, sections_viewed)\n",
    "    graph_stop_events(video, sections_viewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_dist_seeks(all_seek_forwards_ratios, all_seek_backwards_ratios, no_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spikes(sections_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_start_times(all_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist_of_starts(all_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_per_user, sections_aggregated, invalid_data = get_sections_viewed(ai_edx_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_starts = get_all_starts(user_urls_dfs)\n",
    "all_seek_forwards, all_seek_backwards, all_seek_forwards_ls, \\\n",
    "all_seek_backwards_ls, all_seek_forwards_ratios, \\\n",
    "all_seek_backwards_ratios = get_seeks_info(sections_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.to_file = False\n",
    "for video, sections_viewed in sections_aggregated.items():\n",
    "    graph_complexity(video, sections_viewed)\n",
    "    graph_stop_events(video, sections_viewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dist_seeks(all_seek_forwards_ratios, all_seek_backwards_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spikes(sections_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist_of_starts(all_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
