{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "import sys\n",
    "print(os.path.abspath(''))\n",
    "sys.path.append(os.path.abspath(''))\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [GRADE_PATH, ID_MAP_FILE_PATH, APPINVENTOR_FILE_PATH, EDX_CLICKSTREAM_FILE_PATH]:\n",
    "    if not os.path.exists(f):\n",
    "        %run link_edx_appinventor.ipynb\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsection(resource):\n",
    "    return subsection_of_resources[resource]\n",
    "\n",
    "\n",
    "def get_subsection_resources(resource=None, subsection=None):\n",
    "    if subsection and not resource:\n",
    "        return resources_by_subsection[subsection]\n",
    "    elif resource and not subsection:\n",
    "        return resources_by_subsection[get_subsection(resource)]\n",
    "    else:\n",
    "        print(\"Both resource and subsection are none\")\n",
    "\n",
    "        \n",
    "def update_display_names_and_tabs(display_names, tabs, display_name, tab, subsection, tabs_col):\n",
    "    display_names.append(display_name)\n",
    "    tabs[subsection] = tab\n",
    "    tabs_col.append(tab)\n",
    "\n",
    "def should_ignore(user, reset_ignored_users=False):\n",
    "    if not hasattr(should_ignore, 'ignored_users') or reset_ignored_users:\n",
    "        should_ignore.ignored_users = []\n",
    "    \n",
    "    if user in users_to_ignore.username.values:\n",
    "        print(\"Ignoring {} user {}\".format(\n",
    "            users_to_ignore[users_to_ignore.username == user][COHORT_NAME].values[0], \n",
    "            user)\n",
    "        )\n",
    "        \n",
    "        should_ignore.ignored_users.append(user)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_user_urls_dfs():\n",
    "    user_urls_dfs = []\n",
    "    \n",
    "    for user, rows in grouped:\n",
    "        if should_ignore(user): continue\n",
    "\n",
    "        times = pd.to_datetime(rows['time'])\n",
    "\n",
    "        pages = rows['page']\n",
    "\n",
    "        event_types = rows['event_type']\n",
    "        \n",
    "        events = rows['event']\n",
    "\n",
    "        user_url = pd.DataFrame(\n",
    "            zip(times, pages, event_types, events), \n",
    "            columns=['time', 'page', 'event_type', 'event']\n",
    "        ).dropna(subset=['page'])\n",
    "\n",
    "        # Check if empty after removing NaN values\n",
    "        if not user_url.size: continue\n",
    "\n",
    "        user_url = user_url.sort_values(by='time')\n",
    "\n",
    "        display_names = []\n",
    "        tabs_col = []\n",
    "\n",
    "        tab_num = 1\n",
    "        most_recent_tabs = collections.defaultdict(lambda: 1)\n",
    "\n",
    "        for index, row in user_url.iterrows():\n",
    "            p = row.page\n",
    "            \n",
    "            try:\n",
    "                p_id = p.split('/')[-2]\n",
    "\n",
    "            # If no display name found, append page url id\n",
    "            except IndexError:\n",
    "                display_names.append(p)\n",
    "                tabs_col.append(-1)\n",
    "                continue\n",
    "            \n",
    "            # Get the incorrect display name, which we will soon correct\n",
    "            display_name = course_df[course_df[FILE_NAME] == p_id][DISPLAY_NAME].values            \n",
    "            \n",
    "            try:\n",
    "                display_name = display_name[0]\n",
    "            except IndexError:\n",
    "                display_names.append(p_id)\n",
    "                tabs_col.append(-1)\n",
    "                continue\n",
    "            \n",
    "            if display_name in HOMEPAGE_NAMES:\n",
    "                display_names.append(display_name)\n",
    "                tabs_col.append(0)\n",
    "                continue\n",
    "                \n",
    "            subsection = get_subsection(display_name)\n",
    "            subsection_resources = get_subsection_resources(subsection=subsection)\n",
    "            \n",
    "            try:\n",
    "                last_part = int(p.split('/')[-1])\n",
    "            except ValueError:\n",
    "                last_part = None\n",
    "            \n",
    "            try:\n",
    "                extra_display_name = extra_urls[extra_urls.url == p].display_name.iloc[0]\n",
    "            except IndexError:\n",
    "                extra_display_name = None \n",
    "                \n",
    "            # Get urls that come from the homepage\n",
    "            if extra_display_name:\n",
    "                assert subsection == get_subsection(extra_display_name)\n",
    "\n",
    "                tab_num = get_subsection_resources(subsection=subsection).index(extra_display_name) + 1\n",
    "                \n",
    "                update_display_names_and_tabs(display_names, most_recent_tabs, extra_display_name, tab_num, subsection, tabs_col)\n",
    "\n",
    "            # Get links which end with their tab number\n",
    "            elif last_part and last_part <= len(subsection_resources) and last_part >= 0:\n",
    "                display_name = subsection_resources[last_part - 1]\n",
    "                tab_num = last_part\n",
    "\n",
    "                update_display_names_and_tabs(display_names, most_recent_tabs, display_name, tab_num, subsection, tabs_col)\n",
    "            \n",
    "            # Otherwise get the most recently accessed resource \n",
    "            else:\n",
    "                tab_num = most_recent_tabs[subsection] - 1\n",
    "                display_name = subsection_resources[tab_num]            \n",
    "                display_names.append(display_name)\n",
    "                tabs_col.append(tab_num + 1)\n",
    "\n",
    "            ## Set new tab num for the next page accessed\n",
    "            \n",
    "            # Navigation within subsection\n",
    "            if row.event_type in UNIT_NAV_EVENTS_OLD:\n",
    "                most_recent_tabs[subsection] = json.loads(row.event)['new']\n",
    "            \n",
    "            # Navigate to next subsection\n",
    "            elif row.event_type == 'edx.ui.lms.sequence.next_selected':\n",
    "                new_subsection = subsection_order[subsection_order.index(subsection) + 1]\n",
    "                most_recent_tabs[new_subsection] = 1\n",
    "            \n",
    "            # Navigate to previous subsection\n",
    "            elif row.event_type == 'edx.ui.lms.sequence.previous_selected':\n",
    "                new_subsection = subsection_order[subsection_order.index(subsection) - 1]\n",
    "                new_subsection_resources = resources_by_subsection[new_subsection]\n",
    "                most_recent_tabs[new_subsection] = len(new_subsection_resources)\n",
    "\n",
    "        user_url['display_name'] = np.array(display_names)\n",
    "        user_url['user'] = user\n",
    "        user_url['tab'] = tabs_col\n",
    "\n",
    "        user_urls_dfs.append(user_url)\n",
    "\n",
    "    return user_urls_dfs\n",
    "\n",
    "def get_uuid(user):\n",
    "    prof_row = student_profile_df.loc[user == student_profile_df.username]\n",
    "    mapped_user = prof_row.mapped_username_on_alfa.values[0]\n",
    "\n",
    "    appin_row = appin_files_df.loc[mapped_user == appin_files_df.username]\n",
    "    uuid = appin_row.uuid.values[0]\n",
    "    \n",
    "    return uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grades\n",
    "grade_df = pd.read_csv(GRADE_PATH)\n",
    "grade_df = grade_df.rename(columns={'Username': USERNAME})\n",
    "\n",
    "print(grade_df[COHORT_NAME].unique())\n",
    "print(grade_df[COHORT_NAME].value_counts())\n",
    "print(type(grade_df[COHORT_NAME].unique()[2]))\n",
    "\n",
    "# Print all values for each column\n",
    "for col in grade_df.columns[3:]:\n",
    "    print(\"{}\\n\".format(grade_df[col].value_counts())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load appinventor files\n",
    "appin_files_df = pd.read_csv(APPINVENTOR_FILE_PATH)\n",
    "display(appin_files_df)\n",
    "\n",
    "print('AppInventor users: {}'.format(appin_files_df[USERNAME].nunique()))\n",
    "print('Number of files per user:')\n",
    "print(appin_files_df[USERNAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load id map\n",
    "student_profile_df = pd.read_csv(ID_MAP_FILE_PATH)\n",
    "display(student_profile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df = pd.read_csv(EDX_CLICKSTREAM_FILE_PATH)\n",
    "clickstream_df = clickstream_df[~clickstream_df.page.isin(IGNORE_RESOURCES)]\n",
    "\n",
    "whitelist = [_ for _ in clickstream_df[EVENT_TYPE].unique() if '/' not in _ and '.' not in _]\n",
    "print(whitelist)\n",
    "print(\"Event types on EDx\")\n",
    "df = clickstream_df[clickstream_df[EVENT_TYPE].isin(whitelist)]\n",
    "print(df[EVENT_TYPE].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get who is in the mapping\n",
    "unique_users = clickstream_df.username.unique()\n",
    "user_matches = {CLICK_ID_MAP: [], ID_MAP_APP: []}\n",
    "\n",
    "for row in student_profile_df.iterrows():\n",
    "    if row[1].username in unique_users:\n",
    "        user_matches[CLICK_ID_MAP].append(row[1].username)\n",
    "        \n",
    "        if row[1].mapped_username_on_alfa in appin_files_df.username.values:\n",
    "            user_matches[ID_MAP_APP].append(row[1].username)\n",
    "\n",
    "print('Clickstream and id_map matches: {}'.format(len(user_matches[CLICK_ID_MAP])))\n",
    "print('Clickstream and id_map and AppInventor matches: {}'.format(len(user_matches[ID_MAP_APP])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_edx_usr_df = grade_df.loc[grade_df[USERNAME].isin(user_matches[ID_MAP_APP])]\n",
    "\n",
    "print(\"Cohort for users with activity on AI and EDx:\")\n",
    "print(ai_edx_usr_df[COHORT_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_ignore = grade_df[grade_df[COHORT_NAME].isin(IGNORE_COHORTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_nodes = []\n",
    "\n",
    "for root, _, files in os.walk(COURSE_PATH):\n",
    "    if files:\n",
    "        parent = os.path.split(root)[-1]\n",
    "\n",
    "    for f in files:\n",
    "        if f.endswith(SUFFIX):\n",
    "            path = os.path.join(root, f)\n",
    "            nodes = get_node_info(path, parent, f)\n",
    "\n",
    "            course_nodes.extend(nodes)\n",
    "                \n",
    "course_df = pd.DataFrame(course_nodes)\n",
    "\n",
    "for _, row in course_df.iterrows():\n",
    "    if row.display_name in display_name_errors:\n",
    "        row.display_name = display_name_errors[row.display_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "\n",
    "for row in clickstream_df.iterrows():\n",
    "    page = row[1]['page']\n",
    "    \n",
    "    if page is np.nan: continue\n",
    "\n",
    "    try:\n",
    "        match = page.split('/')[-2] in course_df[FILE_NAME].values\n",
    "    \n",
    "        if match: matches.append(row[1])\n",
    "\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_urls = pd.read_csv(EXTRA_URLS_CSV, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_urls = pd.read_csv(NAV_URLS_CSV, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_order_df = pd.read_json(RESOURCE_JSON, orient='records').T\n",
    "resource_order = list(resource_order_df.unit.unique())\n",
    "\n",
    "resource_order_df.section = resource_order_df.section.str.replace('\\d+-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_order = list(resource_order_df.section.unique())\n",
    "subsection_order = list(resource_order_df.subsection.unique())\n",
    "resources_by_section = resource_order_df.groupby('section').unit \\\n",
    "                                        .unique().apply(list).to_dict()\n",
    "section_of_resources = {}\n",
    "\n",
    "# Is there a one liner for this?\n",
    "for section, resources in resources_by_section.items():\n",
    "    for resource in resources:\n",
    "        section_of_resources[resource] = section\n",
    "\n",
    "resources_by_subsection = resource_order_df.groupby('subsection').unit \\\n",
    "                                          .unique().apply(list).to_dict()\n",
    "subsection_of_resources = {}\n",
    "\n",
    "# Is there a one liner for this? make this a function\n",
    "for subsection, resources in resources_by_subsection.items():\n",
    "    for resource in resources:\n",
    "        subsection_of_resources[resource] = subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = clickstream_df.groupby(by='username')\n",
    "user_urls_dfs_with_homepage = get_user_urls_dfs()\n",
    "\n",
    "for u in user_urls_dfs_with_homepage:\n",
    "    assert(u.time.is_monotonic_increasing)\n",
    "    \n",
    "user_urls_dfs = [u[~u.display_name.isin(HOMEPAGE_NAMES)] for u in user_urls_dfs_with_homepage]\n",
    "\n",
    "tmp = []\n",
    "\n",
    "# filter out empty dfs\n",
    "for u in user_urls_dfs:\n",
    "    if u.size:\n",
    "        tmp.append(u)\n",
    "\n",
    "user_urls_dfs = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to make sure that resources align with the tab number in their subsection\n",
    "for u in user_urls_dfs_with_homepage:\n",
    "    for _, row in u.iterrows():\n",
    "        if row['display_name'] in HOMEPAGE_NAMES or row['display_name'] in ['search', 'bookmarks']:\n",
    "            continue\n",
    "            \n",
    "        subsection_resources = get_subsection_resources(resource=row['display_name'])\n",
    "        resource_index = subsection_resources.index(row['display_name']) + 1\n",
    "    \n",
    "        if resource_index != row.tab:\n",
    "            print(resource_index, row.tab, row)\n",
    "\n",
    "print(set(users_to_ignore.username.values) - set(should_ignore.ignored_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pages that have no clicks\n",
    "resources_used = pd.concat(user_urls_dfs).display_name.unique()\n",
    "\n",
    "for r in resource_order:\n",
    "    if r not in resources_used:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_edx_usr_df = grade_df.loc[grade_df[USERNAME].isin(user_matches[ID_MAP_APP])]\n",
    "print(\"Cohort for users with activity on AI and EDx:\")\n",
    "print(ai_edx_usr_df[COHORT_NAME].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_times = {}\n",
    "\n",
    "for user, _ in grouped:\n",
    "    if user in ai_edx_usr_df.username.values:\n",
    "        uuid = get_uuid(user)\n",
    "\n",
    "        folder = os.path.join(AI_DATA, uuid)\n",
    "        scm_files = get_backups_by_type(folder, SCM)\n",
    "        bky_files = get_backups_by_type(folder, BKY)\n",
    "        \n",
    "        user_times[user] = {SCM: list(scm_files.keys()), BKY: list(bky_files.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_edx_dfs = []\n",
    "\n",
    "users_no_scm_bky_interaction = 0\n",
    "\n",
    "for uu_df in user_urls_dfs:\n",
    "    user = uu_df.user.iloc[0]\n",
    "    \n",
    "    if user in ai_edx_usr_df.username.values:\n",
    "        # Ignore ones with no scm or bky interaction\n",
    "        if sum([len(list(t)) for t in user_times[user].values()]):\n",
    "            for file_type, times in user_times[user].items():\n",
    "        \n",
    "                for t in times:\n",
    "                    uu_df = uu_df.append({\"time\": t, \"page\": \"\", \"display_name\": \n",
    "                                          file_type, \"user\": user}, ignore_index=True)\n",
    "            \n",
    "            uu_df = uu_df.sort_values(by='time')\n",
    "            uu_df.display_name.reindex(uu_df.time)\n",
    "            ai_edx_dfs.append(uu_df)\n",
    "        \n",
    "        else: \n",
    "            users_no_scm_bky_interaction += 1\n",
    "            \n",
    "for a in ai_edx_dfs:\n",
    "    assert(a.time.is_monotonic_increasing)\n",
    "    \n",
    "print(users_no_scm_bky_interaction)\n",
    "\n",
    "assert users_no_scm_bky_interaction == 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_edx_users = [u.user.iloc[0] for u in ai_edx_dfs]\n",
    "assert(len(set(ai_edx_users)) == len(ai_edx_users))\n",
    "\n",
    "total_appinventor_edx_events = sum([u.shape[0] for u in ai_edx_dfs])\n",
    "print(\"Total for AI and EDX interaction users only: {}\".format(total_appinventor_edx_events))\n",
    "\n",
    "total_course_events = 0\n",
    "for u in user_urls_dfs:\n",
    "    if u.user.iloc[0] not in ai_edx_users: \n",
    "        total_course_events += len(u)\n",
    "\n",
    "print(\"Total for only Course events: {}\".format(total_course_events))\n",
    "print(\"Total for All AppInventor File Events: {}\".format(total_appinventor_edx_events + total_course_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this copy is necessary\n",
    "all_resource_dfs = ai_edx_dfs.copy()\n",
    "ai_edx_dfs_users = [u.user.iloc[0] for u in ai_edx_dfs]\n",
    "\n",
    "for u in user_urls_dfs:\n",
    "    if u.user.iloc[0] not in ai_edx_dfs_users:\n",
    "        all_resource_dfs.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this is necessary\n",
    "for u in user_urls_dfs:\n",
    "    u['day'] = u.time.apply(lambda t: t.date())\n",
    "    \n",
    "for u in ai_edx_dfs:\n",
    "    u['day'] = u.time.apply(lambda t: t.date())\n",
    "    \n",
    "for u in all_resource_dfs:\n",
    "    u['day'] = u.time.apply(lambda t: t.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_categories = pd.read_csv(CATEGORY_CSV, na_values=['', '\\n'])\n",
    "resource_categories = resource_categories.set_index(CATEGORY_RESOURCE_COLUMN).T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('user_and_course_dfs.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
